{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text (TTT)\n",
    "\n",
    "## 1.1 Setting Up\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n TTT python=3.10.10 -y\n",
    "conda activate TTT\n",
    "pip install -e \".[TTT]\"\n",
    "```\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   anthropic 0.8.1\n",
    "-   transformers 4.36.2\n",
    "-   bitsandbytes 0.41.3.post2\n",
    "-   scipy 1.11.4\n",
    "-   accelerate 0.25.0\n",
    "-   llama-cpp-python 0.2.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Running as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT4\n",
    "\n",
    "For (1) and (2) below, you will use the GaiGen library to call OpenAI's GPT4.\n",
    "You will need to get an API key from OpenAI. \n",
    "Create .env file in project root directory and insert the OpenAI API Key below:\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. GPT4 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}], max_tokens=100,stream=True)\n",
    "#print(response.choices[0].message.content)\n",
    "for message in response:\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. GPT4 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B 8k-context 4-bit quantized\n",
    "\n",
    "For (3) and (4), you will run Mistral 7B locally. Clone TheBloke's 4-bit quantized version of Mistral-7B model from hugging face. This model utilizes the exLlama loader for increased performance. Make sure you have huggingface-hub installed, if not run `pip install huggingface-hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n",
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and the secrets of the universe, and she believed that everything happened for a reason. One day, as she sat on her porch watching the sun set over the mountains, she noticed a young boy playing in the field across the street\n"
     ]
    }
   ],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100, stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and the secrets of the universe, and she had gained a deep understanding of the interconnectedness of all things. One day, as she sat by the riverbank, watching the sun set over the mountains, she felt a sense"
     ]
    }
   ],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yarn-Mistral-7B 128k-context 4-bit quantized\n",
    "\n",
    "Repeat the earlier examples but using a different version of Mistral-7B model with a larger context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir ~/gai/models/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to their paper, the perplexity seems better than the original once the token length is greater than 10k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perplexity-of-mistral7b-128k](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropics Claude2.1\n",
    "\n",
    "The following example uses Anthropics Claude2.1 100k context window size model. Get API Key from Anthropics and add it to the .env file.\n",
    "```sh\n",
    "ANTHROPIC_APIKEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Claude-2.1 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Claude-2.1 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B with HuggingFace transformers\n",
    "\n",
    "Follow the instructions [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama2) to signup with Meta to download the LLaMa-2 model.\n",
    "Download the model in HuggingFace format from [here] (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) into ~/gai/models/Llama-2-7b-chat-hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Llama2-7B Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Llama2-7B Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B GGUF with LlaMaCPP (CPU only)\n",
    "\n",
    "The following example uses GGUF formatted version of Mistral-7B for LlaMaCPP. This can be used when you want the model to run off CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the model\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                mistral-7b-instruct-v0.1.Q4_K_M.gguf  \\\n",
    "                config.json \\\n",
    "                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "import sys\n",
    "with io.capture_output() as captured:\n",
    "    # Redirect stderr to stdout\n",
    "    sys.stderr = sys.stdout    \n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Using Function Call\n",
    "\n",
    "OpenAPI provided a powerful feature for its API called Function calling. It is essentially a way for the LLM to seek external help when encountering limitation to its ability to generate text but returning a string emulating the calling of a function based on the function description provied by the user.\n",
    "\n",
    "We extends this feature to the open source models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1109f336-a568-4594-a944-6dd2208382cd', function=Function(arguments=' {\\n            \"search_query\": \"current date\"\\n        }', name='gg'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "\n",
    "import json\n",
    "system_prompt = \"\"\"\n",
    "        You will always begin your interaction by asking yourself if the user's message is a message that requires a tool response or a text response.\n",
    "                        \n",
    "        DEFINITIONS:\n",
    "        1. A tool response is based on the following JSON format:\n",
    "                <tool>\n",
    "                {{\n",
    "                    'type':'tool',\n",
    "                    'tool': ...\n",
    "                }}\n",
    "                </tool>\n",
    "        \n",
    "           And the tool is chosen from the following <tools> list:\n",
    "                <tools>\n",
    "                {tools}\n",
    "                </tools>.\n",
    "            \n",
    "        2. A text response is based on the following JSON format:\n",
    "                <text>\n",
    "                {{\n",
    "                    'type':'text',\n",
    "                    'text': ...\n",
    "                }}\n",
    "                </text>\n",
    "        \n",
    "        STEPS:\n",
    "        1. Think about the nature of the user's message.\n",
    "            * Is the user's message a question that I can answer factually within my knowledge domain?\n",
    "            * Are there any dependencies to external factors that I need to consider before answering the user's question?\n",
    "            * What are the tools I have at my disposal to help me answer the user's question? \n",
    "        2. If the user's message requires a tool response, pick the most suitable tool response from <tools>. \n",
    "            * I can refer to the \"description\" field of each tool to help me decide.\n",
    "            * For example, if I need to search for real-time information, I can use the \"gg\" tool and if I know where to find the information, I can use the \"scrape\" tool.\n",
    "        3. If the user's message does not require a tool response, provide a text response to the user.\n",
    "\n",
    "        CONSTRAINTS:        \n",
    "        1. You can only provide a tool response or a text response and nothing else.\n",
    "        2. When providing a tool response, respond only in JSON and only pick from <tools>. That means, begin your message with a curly bracket ' and end your message with a curly bracket '. Do not respond with anything else.\n",
    "        3. Remember, do not invent your own tools. You can only pick from <tools>.\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"scrape\",\n",
    "            \"description\": \"Scrape the content of the provided url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url to scrape the content from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "# user_prompt = \"Who is the current president of singapore?\"\n",
    "# user_prompt = \"Tell me the latest news on Singapore\"\n",
    "# user_prompt = \"Tell me a one paragraph short story.\"\n",
    "user_prompt = \"What is today's date?\"\n",
    "\n",
    "response = gen.create(messages=[{'role':'system','content':system_prompt.format(tools=json.dumps(tools))},\n",
    "                                {'role':'user','content':user_prompt},\n",
    "                                {'role':'assistant','content':''}],\n",
    "                    stream=False,\n",
    "                    max_new_tokens=100)\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Stop any container with the same name\n",
    "docker rm -f gai-ttt\n",
    "\n",
    "# Start the container\n",
    "docker run -d \\\n",
    "    --name gai-ttt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-ttt:latest\n",
    "\n",
    "# Wait for model to load\n",
    "sleep 30\n",
    "\n",
    "# Confirm its running\n",
    "docker logs gai-ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 2: Run Text Generation Client\n",
    "\n",
    "The default model is Mistral7B-8k context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a one paragraph short story.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": True\n",
    "    },\n",
    "    stream=True)\n",
    "for chunk in response.iter_lines():\n",
    "    result = json.loads(chunk.decode('utf-8'))\n",
    "    print(result[\"choices\"][0][\"delta\"][\"content\"],end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "system_prompt = \"\"\"\n",
    "        You will always begin your interaction by asking yourself if the user's message is a message that requires a tool response or a text response.\n",
    "                        \n",
    "        DEFINITIONS:\n",
    "        1. A tool response is based on a tool chosen from the <tools> list below:\n",
    "                <tools>\n",
    "                {tools}\n",
    "                </tools>.\n",
    "            \n",
    "        2. A text response is a normal text response that you would provide to the user, formatted as JSON:\n",
    "                <text>\n",
    "                    {{\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": ...\n",
    "                    }}\n",
    "                <text>\n",
    "        \n",
    "        STEPS:\n",
    "        1. Think about the nature of the user's message.\n",
    "            * Is the user's message a question that I can answer factually within my knowledge domain?\n",
    "            * Are there any dependencies to external factors that I need to consider before answering the user's question?\n",
    "            * What are the tools I have at my disposal to help me answer the user's question? \n",
    "        2. If the user's message requires a tool response, pick the most suitable tool response from <tools>. \n",
    "            * I can refer to the \"description\" field of each tool to help me decide.\n",
    "            * For example, if I need to search for real-time information, I can use the \"gg\" tool and if I know where to find the information, I can use the \"scrape\" tool.\n",
    "        3. If the user's message does not require a tool response, provide a text response to the user.\n",
    "\n",
    "        CONSTRAINTS:        \n",
    "        1. You can only provide a tool response or a text response and nothing else.\n",
    "        2. When providing a tool response, respond only in JSON and only pick from <tools>. That means, begin your message with a curly bracket ' and end your message with a curly bracket '. Do not respond with anything else.\n",
    "        3. Remember, do not invent your own tools. You can only pick from <tools>.\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"Search google based on the provided search query\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"scrape\",\n",
    "            \"description\": \"Scrape the content of the provided url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url to scrape the content from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "#user_prompt = \"Tell me the latest news on Singapore\"\n",
    "\n",
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt.format(tools=json.dumps(tools))},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": False\n",
    "    })\n",
    "if (response.status_code!=200):\n",
    "    raise Exception(response.text)\n",
    "\n",
    "print(json.loads(response.text)['choices'][0]['message']['tool_calls'][0]['function']['name'])\n",
    "print(json.loads(response.text)['choices'][0]['message']['tool_calls'][0]['function']['arguments'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
