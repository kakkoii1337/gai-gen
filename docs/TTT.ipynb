{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text (TTT)\n",
    "\n",
    "## 1.1 Setting Up\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n TTT python=3.10.10 -y\n",
    "conda activate TTT\n",
    "pip install -e \".[TTT]\"\n",
    "```\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   anthropic 0.8.1\n",
    "-   transformers 4.36.2\n",
    "-   bitsandbytes 0.41.3.post2\n",
    "-   scipy 1.11.4\n",
    "-   accelerate 0.25.0\n",
    "-   llama-cpp-python 0.2.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Running as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT4\n",
    "\n",
    "For (1) and (2) below, you will use the GaiGen library to call OpenAI's GPT4.\n",
    "You will need to get an API key from OpenAI. \n",
    "Create .env file in project root directory and insert the OpenAI API Key below:\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. GPT4 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. GPT4 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B 8k-context 4-bit quantized\n",
    "\n",
    "For (3) and (4), you will run Mistral 7B locally. Clone TheBloke's 4-bit quantized version of Mistral-7B model from hugging face. This model utilizes the exLlama loader for increased performance. Make sure you have huggingface-hub installed, if not run `pip install huggingface-hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100, stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yarn-Mistral-7B 128k-context 4-bit quantized\n",
    "\n",
    "Repeat the earlier examples but using a different version of Mistral-7B model with a larger context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir ~/gai/models/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to their paper, the perplexity seems better than the original once the token length is greater than 10k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perplexity-of-mistral7b-128k](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n",
      "Once upon a time, there was a little girl who loved to play outside in the sunshine. One day she went out into her backyard and saw something strange – it looked like a giant spider web! She walked closer and realized that it wasn’t a spider at all but rather an intricate pattern of leaves on the ground. The more she explored this beautiful design, the happier she became until finally she sat down under its shade for hours enjoying nature’s beauty around her"
     ]
    }
   ],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      " Once upon a time, there was a little girl who loved to play outside in the sunshine. One day she went out into her backyard and saw something strange – it looked like a giant spider web! She walked closer and realized that it wasn’t a spider at all but rather an intricate pattern of leaves on the ground. The more she explored this beautiful design, the happier she became until finally she sat down under its shade for hours enjoying nature’s beauty around her"
     ]
    }
   ],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropics Claude2.1\n",
    "\n",
    "The following example uses Anthropics Claude2.1 100k context window size model. Get API Key from Anthropics and add it to the .env file.\n",
    "```sh\n",
    "ANTHROPIC_APIKEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Claude-2.1 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Claude-2.1 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B with HuggingFace transformers\n",
    "\n",
    "Follow the instructions [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama2) to signup with Meta to download the LLaMa-2 model.\n",
    "Download the model in HuggingFace format from [here] (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) into ~/gai/models/Llama-2-7b-chat-hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Llama2-7B Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Llama2-7B Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B GGUF with LlaMaCPP (CPU only)\n",
    "\n",
    "The following example uses GGUF formatted version of Mistral-7B for LlaMaCPP. This can be used when you want the model to run off CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the model\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                mistral-7b-instruct-v0.1.Q4_K_M.gguf  \\\n",
    "                config.json \\\n",
    "                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "import sys\n",
    "with io.capture_output() as captured:\n",
    "    # Redirect stderr to stdout\n",
    "    sys.stderr = sys.stdout    \n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Stop any container with the same name\n",
    "docker rm -f gai-ttt\n",
    "\n",
    "# Start the container\n",
    "docker run -d \\\n",
    "    --name gai-ttt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-ttt:latest\n",
    "\n",
    "# Wait for model to load\n",
    "sleep 30\n",
    "\n",
    "# Confirm its running\n",
    "docker logs gai-ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 2: Test using Mistral7B-8k\n",
    "\n",
    "The default model is Mistral7B-8k context size. You can test it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#### Step 2: Test\n",
    "\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"mistral7b-exllama\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"max_new_tokens\\\":100, \\\n",
    "        \\\"stream\\\":true}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Change the default model to Mistral7B-128k\n",
    "\n",
    "Change the default model to Mistral7b-128k by changing the config file in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Change default model\n",
    "curl -X PUT \\\n",
    "    http://localhost:12031/gen/v1/chat/config \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -d '{\"generator_name\":\"mistral7b_128k-exllama\",                       \n",
    "        \"generator_config\": {                               \n",
    "            \"type\": \"ttt\",\n",
    "            \"model_name\": \"Mistral7B_128k-ExLlama\",\n",
    "            \"engine\": \"ExLlamaV2_TTT\",\n",
    "            \"model_path\": \"models/Yarn-Mistral-7B-128k-GPTQ\",\n",
    "            \"model_basename\": \"model\",\n",
    "            \"max_seq_len\": 65536,\n",
    "            \"stopping_words\": [],\n",
    "            \"hyperparameters\": {\n",
    "                \"temperature\": 1.2,\n",
    "                \"top_p\": 0.15,\n",
    "                \"min_p\": 0.0,\n",
    "                \"top_k\": 50,\n",
    "                \"max_new_tokens\": 1000,\n",
    "                \"typical\": 0.0,\n",
    "                \"token_repetition_penalty_max\": 1.25,\n",
    "                \"token_repetition_penalty_sustain\": 256,\n",
    "                \"token_repetition_penalty_decay\": 128,\n",
    "                \"beams\": 1,\n",
    "                \"beam_length\": 1\n",
    "            }\n",
    "        }\n",
    "    }'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart the container\n",
    "\n",
    "```bash\n",
    "# Restart container\n",
    "docker stop gai-ttt\n",
    "docker start gai-ttt\n",
    "```\n",
    "\n",
    "Wait for the model to load and check the logs to confirm that the new model is loaded.\n",
    "\n",
    "```bash\n",
    "docker logs gai-ttt\n",
    "```\n",
    "\n",
    "### Step 4. Test Again using Mistral7B-128k\n",
    "\n",
    "Then test again with mistral7b-128k model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo STREAMING\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"mistral7b_128k-exllama\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"stream\\\":true}\" \\\n",
    "        | python ../print_delta.py\n",
    "echo\n",
    "echo NON-STREAMING\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"mistral7b_128k-exllama\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ]}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
