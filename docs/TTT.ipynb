{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text (TTT)\n",
    "\n",
    "### 1.1 Create Virtual Environment and Install Dependencies\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n TTT python=3.10.10 -y\n",
    "conda activate TTT\n",
    "pip install gai-gen[TTT]\n",
    "```\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   anthropic 0.8.1\n",
    "-   transformers 4.36.2\n",
    "-   bitsandbytes 0.41.3.post2\n",
    "-   scipy 1.11.4\n",
    "-   accelerate 0.25.0\n",
    "-   llama-cpp-python 0.2.25\n",
    "\n",
    "### 1.2 Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demo uses OpenAI API. You will need to get an API key from OpenAI. \n",
    "Create .env file in project root directory and insert the OpenAI API Key below:\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. GPT4 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. GPT4 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this example, clone TheBloke's 4-bit quantized version of Mistral-7B model from hugging face. This model utilizes the exLlama loader for increased performance.\n",
    "\n",
    "```sh\n",
    "git clone https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses Anthropics Claude2.1 100k context window size model. Get API Key from Anthropics and add it to the .env file.\n",
    "```sh\n",
    "ANTHROPIC_APIKEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Claude-2.1 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Claude-2.1 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama2) to signup with Meta to download the LLaMa-2 model.\n",
    "Download the model in HuggingFace format from [here] (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) into ~/gai/models/Llama-2-7b-chat-hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Llama2-7B Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Llama2-7B Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses GGUF formatted version of Mistral-7B for LlaMaCPP. This can be used when you want the model to run off CPU only.\n",
    "Follow this instruction to download TheBloke's Mistral-7B GGUF model:\n",
    "```\n",
    "mkdir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF && cd $_\n",
    "wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "    --name gai-ttt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-ttt:latest\n",
    "```\n",
    "\n",
    "### Step 2: Wait for model to load\n",
    "\n",
    "```bash\n",
    "docker logs gai-ttt\n",
    "```\n",
    "\n",
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#### Step 3: Test\n",
    "\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"mistral7b-exllama\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"max_new_tokens\\\":25,\n",
    "        \\\"stream\\\":true}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
