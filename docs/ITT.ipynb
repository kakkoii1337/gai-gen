{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Image-to-Text (ITT)\n",
    "\n",
    "## 1. Note\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   llava 1.1.3\n",
    "\n",
    "## 2. Create Virtual Environment and Install Dependencies\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n ITT python=3.10.10 -y\n",
    "conda activate ITT\n",
    "pip install gai-gen[ITT]\n",
    "```\n",
    "\n",
    "## 3. Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE:\n",
      "The image shows a bus stop sign, with various elements providing information to passengers. There's a blue rectangular sign at the top with a bus icon, symbolizing that it's a bus stop. Below it, there is a sign with a bus stop number (71091) and what appears to be a code (BIK 637).\n",
      "\n",
      "Beside these signs, there is a schedule board with lists of numbers in red and white, which are likely the bus service numbers that stop at this location. The numbers are organized in a grid, possibly grouped by the route or schedule, to inform passengers about their options for public transportation at this stop.\n",
      "\n",
      "In the background, there are trees, indicating that the bus stop is situated in an area with greenery. The overall scene suggests this is a bus stop intended for regular public use, possibly in an urban or suburban area.\n"
     ]
    }
   ],
   "source": [
    "## 6.17 OpenAI Vision Image-to-Text Generation\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('openai-vision')\n",
    "\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"GENERATE:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300\n",
    "  )\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      "The image displays a bus stop sign, indicating the location where buses pick up passengers. The sign shows several elements:\n",
      "\n",
      "1. A blue rectangle with a white bus symbol on top, clearly marking that the sign is for a bus stop.\n",
      "2. A sign with a bus stop number \"71091\", along with \"Blk 637\" which likely refers to the nearby building or block number.\n",
      "3. A list of bus numbers on panels underneath, showing the different bus services that stop at this location. \n",
      "4. Two direction arrows, possibly indicating the direction in which the buses will be going, or the locations of nearby bus stops.\n",
      "\n",
      "The greenery in the background suggests that the bus stop is located in an area with trees, likely providing some shade for waiting passengers."
     ]
    }
   ],
   "source": [
    "## 6.18 OpenAI Vision Image-to-Text Streaming\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('openai-vision')\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    "  stream=True\n",
    "  )\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "      print(chunk.choices[0].delta.content,end=\"\",flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demo is uses the LLaVa model, installation instructions can be found [here](./docs/8-llava-transformers.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## 6.19 LlaVa Image-to-Text Generation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Gaigen\n\u001b[0;32m----> 4\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mGaigen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetInstance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllava-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[1;32m      7\u001b[0m encoded_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/github/kakkoii1337/gai-aio/gai/gen/Gaigen.py:53\u001b[0m, in \u001b[0;36mGaigen.load\u001b[0;34m(self, generator_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mitt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ITT\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[43mITT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaigen.load: The generator_type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerator_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/kakkoii1337/gai-aio/gai/gen/itt/ITT.py:10\u001b[0m, in \u001b[0;36mITT.__init__\u001b[0;34m(self, generator_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m generators_utils\u001b[38;5;241m.\u001b[39mload_generators_config()[generator_name]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLlava_ITT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mitt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLlava_ITT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llava_ITT\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitt \u001b[38;5;241m=\u001b[39m Llava_ITT(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVision_ITT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/github/kakkoii1337/gai-aio/gai/gen/itt/Llava_ITT.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_images \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m conv_templates\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llava'"
     ]
    }
   ],
   "source": [
    "## 6.19 LlaVa Image-to-Text Generation\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llava-transformers')\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"GENERATE:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_new_tokens=300\n",
    "  )\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e7ebced0b14a43b0ba6e982cca3af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      "The image features a street scene with a bus stop sign and a bus route sign. The bus stop sign is located on a pole, while the bus route sign is hanging on the side of the pole. The bus route sign displays the number 7101, indicating the bus route.\n",
      "\n",
      "There are several trees in the background, adding to the urban atmosphere. A person can be seen in the scene, possibly waiting for the bus or just passing by.by."
     ]
    }
   ],
   "source": [
    "## 6.20 LlaVa Image-to-Text Streaming\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llava-transformers')\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_new_tokens=300,\n",
    "  stream=True\n",
    "  )\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end=\"\",flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
