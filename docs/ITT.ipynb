{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Image-to-Text (ITT)\n",
    "\n",
    "## 1. Note\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   llava 1.1.3\n",
    "\n",
    "## 2. Create Virtual Environment and Install Dependencies\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n ITT python=3.10.10 -y\n",
    "conda activate ITT\n",
    "pip install \".[ITT]\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download liuhaotian/llava-v1.5-7b \\\n",
    "        --local-dir ~/gai/models/llava-v1.5-7b \\\n",
    "        --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.17 OpenAI Vision Image-to-Text Generation\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('openai-vision')\n",
    "\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/gen/itt/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"GENERATE:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300\n",
    "  )\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.18 OpenAI Vision Image-to-Text Streaming\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('openai-vision')\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    "  stream=True\n",
    "  )\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "      print(chunk.choices[0].delta.content,end=\"\",flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demo is uses the LLaVa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.19 LlaVa Image-to-Text Generation\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llava-transformers')\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"GENERATE:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_new_tokens=300\n",
    "  )\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.20 LlaVa Image-to-Text Streaming\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llava-transformers')\n",
    "\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/gen/itt/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "response = gen.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_new_tokens=300,\n",
    "  stream=True\n",
    "  )\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end=\"\",flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Llava model, but in huggingface format that can be directly loaded using pure Transformers API. Pro: More consistent for codebase. Con: Less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install the model\n",
    "huggingface-cli download llava-hf/llava-1.5-7b-hf \\\n",
    "        --local-dir ~/gai/models/llava-1.5-7b-hf \\\n",
    "        --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This requires the latest transformer version so activate ITT2 environment \n",
    "pip install \".[ITT2]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roylai/miniconda/envs/ITT2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 12.01s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "What are the numbers in the bus stop sign?\n",
      "ASSISTANT: The numbers in the bus stop sign are 7101.\n"
     ]
    }
   ],
   "source": [
    "## Generating Text from Images\n",
    "from gai.gen.itt.Transformers_ITT import Transformers_ITT\n",
    "config = {\n",
    "    \"type\": \"itt\",\n",
    "    \"engine\": \"Transformers_ITT\",\n",
    "    \"model_path\": \"models/llava-1.5-7b-hf\",\n",
    "    \"model_name\": \"llava-1.5-7b\",\n",
    "}\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"../tests/gen/itt/buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "itt = Transformers_ITT(config)\n",
    "response = itt.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What are the numbers in the bus stop sign?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  stream=False\n",
    "  )\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "    --name gai-itt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-itt:latest\n",
    "```\n",
    "\n",
    "### Step 2: Wait for model to load\n",
    "\n",
    "```bash\n",
    "docker logs gai-itt\n",
    "```\n",
    "\n",
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 3: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "def file_to_data_url(filename):\n",
    "    # Get the mime type of the file\n",
    "    mime_type, _ = mimetypes.guess_type(filename)\n",
    "\n",
    "    # Read the file in binary mode\n",
    "    with open(filename, 'rb') as f:\n",
    "        encoded_string = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    # Format the data url\n",
    "    data_url = f\"data:{mime_type};base64,{encoded_string}\"\n",
    "\n",
    "    return data_url\n",
    "\n",
    "# Load the image file and convert it to a data url\n",
    "data_url = file_to_data_url('../tests/mr-lion-sketch.png')\n",
    "\n",
    "prompt = {\n",
    "    \"max_new_tokens\":1000,\n",
    "    \"stream\":True,\n",
    "    \"messages\":[{\"role\":\"user\",\"content\":[{\"type\": \"text\", \"text\": \"What’s in this image?\"},{\"type\":\"image_url\",\"image_url\":{\"url\":data_url}}]}]\n",
    "    }\n",
    "import json\n",
    "import requests\n",
    "\n",
    "response = requests.post(\"http://localhost:12031/gen/v1/vision/completions\", json=prompt)\n",
    "for chunk in response.iter_lines():\n",
    "    chunk = json.loads(chunk.decode())\n",
    "    print(chunk[\"choices\"][0][\"delta\"][\"content\"],end=\"\",flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
